#!/usr/bin/env python
"""Train GATVerifier on synthetic anomaly data.

This script provides a CLI interface to train the GATVerifier model on
synthetic anomaly data generated by SyntheticAnomalyDataset.

Usage:
    python scripts/train_gnn_verifier.py --epochs 100 --output data/derived/models/gnn/gnn_verifier_v1.pth

Example with custom hyperparameters:
    python scripts/train_gnn_verifier.py \
        --epochs 100 \
        --train-samples 2000 \
        --val-samples 400 \
        --batch-size 32 \
        --lr 0.001 \
        --hidden 64 \
        --seed 42 \
        --output data/derived/models/gnn/gnn_verifier_v1.pth
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from pathlib import Path

# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from fyp.gnn import GATVerifier, GNNTrainer, SyntheticAnomalyDataset


def setup_logging(verbose: bool = False) -> logging.Logger:
    """Configure logging for training script.

    Args:
        verbose: If True, set DEBUG level; else INFO level

    Returns:
        Configured logger instance
    """
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    return logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments.

    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        description="Train GATVerifier model on synthetic anomaly data",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Training hyperparameters
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Number of training epochs",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Batch size for training",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-3,
        help="Learning rate for Adam optimizer",
    )
    parser.add_argument(
        "--weight-decay",
        type=float,
        default=1e-4,
        help="L2 regularization strength",
    )

    # Model hyperparameters
    parser.add_argument(
        "--hidden",
        type=int,
        default=64,
        help="Hidden channels in GATVerifier",
    )
    parser.add_argument(
        "--layers",
        type=int,
        default=3,
        help="Number of GAT layers",
    )
    parser.add_argument(
        "--heads",
        type=int,
        default=4,
        help="Number of attention heads",
    )
    parser.add_argument(
        "--dropout",
        type=float,
        default=0.1,
        help="Dropout rate",
    )

    # Dataset parameters
    parser.add_argument(
        "--train-samples",
        type=int,
        default=2000,
        help="Number of training samples",
    )
    parser.add_argument(
        "--val-samples",
        type=int,
        default=400,
        help="Number of validation samples",
    )
    parser.add_argument(
        "--num-nodes",
        type=int,
        default=44,
        help="Nodes per graph (matches SSEN test data)",
    )
    parser.add_argument(
        "--temporal-features",
        type=int,
        default=5,
        help="Temporal feature dimension",
    )
    parser.add_argument(
        "--anomaly-ratio",
        type=float,
        default=0.5,
        help="Fraction of samples with anomalies",
    )

    # Training control
    parser.add_argument(
        "--early-stopping",
        type=int,
        default=10,
        help="Early stopping patience (epochs without improvement)",
    )
    parser.add_argument(
        "--log-every",
        type=int,
        default=10,
        help="Log training progress every N epochs",
    )

    # Output
    parser.add_argument(
        "--output",
        type=str,
        default="data/derived/models/gnn/gnn_verifier_v1.pth",
        help="Path to save trained model checkpoint",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose (DEBUG) logging",
    )

    return parser.parse_args()


def main() -> int:
    """Main training entry point.

    Returns:
        Exit code (0 for success, 1 for failure)
    """
    args = parse_args()
    logger = setup_logging(args.verbose)

    logger.info("=" * 60)
    logger.info("GATVerifier Training Script")
    logger.info("=" * 60)

    # Log configuration
    logger.info(f"Training Configuration:")
    logger.info(f"  Epochs: {args.epochs}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.lr}")
    logger.info(f"  Weight decay: {args.weight_decay}")
    logger.info(f"  Hidden channels: {args.hidden}")
    logger.info(f"  GAT layers: {args.layers}")
    logger.info(f"  Attention heads: {args.heads}")
    logger.info(f"  Dropout: {args.dropout}")
    logger.info(f"Dataset Configuration:")
    logger.info(f"  Training samples: {args.train_samples}")
    logger.info(f"  Validation samples: {args.val_samples}")
    logger.info(f"  Nodes per graph: {args.num_nodes}")
    logger.info(f"  Temporal features: {args.temporal_features}")
    logger.info(f"  Anomaly ratio: {args.anomaly_ratio}")
    logger.info(f"  Random seed: {args.seed}")
    logger.info(f"Output: {args.output}")
    logger.info("-" * 60)

    # Create output directory
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Create datasets
    logger.info(f"Creating synthetic datasets...")
    train_dataset = SyntheticAnomalyDataset(
        num_samples=args.train_samples,
        num_nodes=args.num_nodes,
        temporal_features=args.temporal_features,
        anomaly_ratio=args.anomaly_ratio,
        seed=args.seed,
    )
    val_dataset = SyntheticAnomalyDataset(
        num_samples=args.val_samples,
        num_nodes=args.num_nodes,
        temporal_features=args.temporal_features,
        anomaly_ratio=args.anomaly_ratio,
        seed=args.seed + 1000,  # Different seed for validation
    )

    logger.info(f"  Training dataset: {len(train_dataset)} samples")
    logger.info(f"  Validation dataset: {len(val_dataset)} samples")
    logger.info(f"  Training anomaly distribution: {train_dataset.get_anomaly_statistics()}")

    # Create model
    model = GATVerifier(
        temporal_features=args.temporal_features,
        hidden_channels=args.hidden,
        num_layers=args.layers,
        heads=args.heads,
        dropout=args.dropout,
    )
    logger.info(f"Created model: {model}")

    # Count parameters
    num_params = sum(p.numel() for p in model.parameters())
    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"  Total parameters: {num_params:,}")
    logger.info(f"  Trainable parameters: {num_trainable:,}")

    # Create trainer
    trainer = GNNTrainer(
        model=model,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        checkpoint_dir=str(output_path.parent),
    )

    # Train
    logger.info("-" * 60)
    logger.info(f"Starting training for {args.epochs} epochs...")
    history = trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        early_stopping_patience=args.early_stopping,
        log_every=args.log_every,
    )

    # Save final model
    trainer.save_checkpoint(str(output_path), args.epochs, history)
    logger.info(f"Saved trained model to {output_path}")

    # Save training history as JSON
    history_path = output_path.with_suffix(".json")
    with open(history_path, "w") as f:
        json.dump(history, f, indent=2)
    logger.info(f"Saved training history to {history_path}")

    # Report final metrics
    logger.info("-" * 60)
    logger.info("TRAINING COMPLETE")
    logger.info("-" * 60)

    if history.get("val_accuracy"):
        final_acc = history["val_accuracy"][-1]
        best_acc = history.get("best_val_accuracy", max(history["val_accuracy"]))
        best_epoch = history.get("best_epoch", 0)

        logger.info(f"Final validation accuracy: {final_acc:.4f} ({final_acc * 100:.2f}%)")
        logger.info(f"Best validation accuracy: {best_acc:.4f} ({best_acc * 100:.2f}%) at epoch {best_epoch}")
        logger.info(f"Final training loss: {history['train_loss'][-1]:.4f}")
        logger.info(f"Final validation loss: {history['val_loss'][-1]:.4f}")

        if history.get("stopped_early"):
            logger.info(f"Training stopped early at epoch {len(history['train_loss'])}")

        # Check against 85% target
        if best_acc >= 0.85:
            logger.info(f"SUCCESS: Best accuracy {best_acc:.2%} >= 85% target")
            return 0
        else:
            logger.warning(f"BELOW TARGET: Best accuracy {best_acc:.2%} < 85% target")
            logger.info("Consider: more epochs, more samples, or different hyperparameters")
            return 0  # Still success (script ran), evaluation will check accuracy

    logger.info("=" * 60)
    return 0


if __name__ == "__main__":
    sys.exit(main())
