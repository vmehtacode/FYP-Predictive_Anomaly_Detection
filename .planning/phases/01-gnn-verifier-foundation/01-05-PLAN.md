---
phase: 01-gnn-verifier-foundation
plan: 05
type: execute
wave: 2
depends_on: ["01-04"]
files_modified:
  - src/fyp/gnn/trainer.py
  - tests/test_gnn/test_trainer.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Training loop trains GATVerifier on synthetic anomaly data"
    - "Model weights are updated through backpropagation"
    - "Training produces decreasing loss over epochs"
    - "Checkpointing saves and loads model state"
  artifacts:
    - path: "src/fyp/gnn/trainer.py"
      provides: "Training pipeline for GATVerifier"
      exports: ["GNNTrainer", "train_gnn_verifier"]
      min_lines: 200
    - path: "tests/test_gnn/test_trainer.py"
      provides: "Unit tests for training pipeline"
      min_lines: 100
  key_links:
    - from: "src/fyp/gnn/trainer.py"
      to: "src/fyp/gnn/gat_verifier.py"
      via: "import + model training"
      pattern: "from fyp.gnn.gat_verifier import GATVerifier"
    - from: "src/fyp/gnn/trainer.py"
      to: "src/fyp/gnn/synthetic_dataset.py"
      via: "import + dataset usage"
      pattern: "from fyp.gnn.synthetic_dataset import SyntheticAnomalyDataset"
    - from: "src/fyp/gnn/trainer.py"
      to: "torch.optim"
      via: "optimizer setup"
      pattern: "torch.optim.Adam"
---

<objective>
Create a training pipeline for the GATVerifier model that trains on synthetic anomaly data and produces model weights achieving >85% accuracy.

Purpose: Close the gap "Training loop with loss function for anomaly detection" - the model architecture exists but has no training infrastructure. This enables achieving the Phase 1 success criterion of >85% accuracy.

Output: GNNTrainer class with complete training loop, loss function, optimizer setup, and checkpointing. Plus a convenience function train_gnn_verifier() for quick training runs.
</objective>

<execution_context>
@/Users/vatsalmehta/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vatsalmehta/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-gnn-verifier-foundation/01-02-SUMMARY.md
@src/fyp/gnn/gat_verifier.py
@src/fyp/gnn/synthetic_dataset.py
@src/fyp/selfplay/trainer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GNNTrainer class with training loop</name>
  <files>src/fyp/gnn/trainer.py</files>
  <action>
Create a GNNTrainer class that trains GATVerifier on synthetic anomaly data:

1. **GNNTrainer class**:
   ```python
   class GNNTrainer:
       def __init__(
           self,
           model: GATVerifier,
           learning_rate: float = 1e-3,
           weight_decay: float = 1e-4,
           device: str = "cpu",
           checkpoint_dir: str = "data/derived/models/gnn",
       ):
   ```

   - Store model, create optimizer (Adam), create loss function (BCELoss for binary anomaly classification)
   - Support both CPU and GPU (torch.device)
   - Create checkpoint directory if not exists

2. **Training loop**:
   ```python
   def train(
       self,
       train_dataset: SyntheticAnomalyDataset,
       val_dataset: SyntheticAnomalyDataset | None = None,
       num_epochs: int = 100,
       batch_size: int = 32,
       early_stopping_patience: int = 10,
       log_every: int = 10,
   ) -> dict:
   ```

   - Create DataLoader from datasets using torch_geometric.loader.DataLoader
   - Training loop with epochs:
     a. Set model.train()
     b. For each batch:
        - Forward pass: scores = model(batch.x, batch.edge_index, batch.node_type)
        - Compute loss: BCELoss(scores.squeeze(), batch.y.float())
        - Backward pass: loss.backward()
        - Optimizer step: optimizer.step()
        - Track running loss
     c. Validation if val_dataset provided:
        - Set model.eval()
        - Compute val loss and accuracy
        - Early stopping check
     d. Logging every log_every epochs
   - Return training history dict

3. **Loss function**:
   - BCELoss for binary node classification (anomaly vs normal)
   - Optional: Focal loss variant for class imbalance
   - Handle class weights if anomaly ratio != 0.5

4. **Checkpointing**:
   ```python
   def save_checkpoint(self, path: str, epoch: int, metrics: dict):
       torch.save({
           'epoch': epoch,
           'model_state_dict': self.model.state_dict(),
           'optimizer_state_dict': self.optimizer.state_dict(),
           'metrics': metrics,
       }, path)

   def load_checkpoint(self, path: str) -> dict:
       checkpoint = torch.load(path)
       self.model.load_state_dict(checkpoint['model_state_dict'])
       self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
       return checkpoint
   ```

5. **Convenience function**:
   ```python
   def train_gnn_verifier(
       temporal_features: int = 5,
       hidden_channels: int = 64,
       num_epochs: int = 100,
       num_train_samples: int = 1000,
       num_val_samples: int = 200,
       **kwargs,
   ) -> tuple[GATVerifier, dict]:
       """Train a GATVerifier from scratch and return trained model + metrics."""
   ```

6. **Metrics tracking**:
   - Train loss per epoch
   - Validation loss per epoch
   - Accuracy (threshold at 0.5)
   - Precision, recall, F1 for anomaly class
   - Best validation accuracy

7. **Export in __init__.py**: Add GNNTrainer, train_gnn_verifier to fyp.gnn exports
  </action>
  <verify>
python -c "from fyp.gnn import GNNTrainer, GATVerifier, SyntheticAnomalyDataset; m = GATVerifier(5); t = GNNTrainer(m); print('GNNTrainer initialized')"
  </verify>
  <done>
GNNTrainer class exists with complete training loop, BCELoss, Adam optimizer, and checkpointing. train_gnn_verifier convenience function available for quick training runs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for training pipeline</name>
  <files>tests/test_gnn/test_trainer.py</files>
  <action>
Create comprehensive tests for the GNN training pipeline:

1. **Initialization tests**:
   - test_trainer_init(): Verify GNNTrainer initializes with model
   - test_trainer_device(): Verify CPU/GPU device handling
   - test_optimizer_created(): Verify Adam optimizer is created
   - test_loss_function(): Verify BCELoss is used

2. **Training tests**:
   - test_single_epoch(): Train for 1 epoch, verify loss computed
   - test_loss_decreases(): Train for 10 epochs, verify loss trend
   - test_model_weights_updated(): Verify weights change after training
   - test_gradient_flow(): Verify gradients computed correctly
   - test_batch_processing(): Verify batches processed correctly

3. **Validation tests**:
   - test_validation_loop(): Verify validation runs after each epoch
   - test_early_stopping(): Verify training stops on patience exhaustion
   - test_validation_metrics(): Verify accuracy, precision, recall computed

4. **Checkpointing tests**:
   - test_save_checkpoint(): Verify checkpoint file created
   - test_load_checkpoint(): Verify model restored from checkpoint
   - test_checkpoint_contents(): Verify checkpoint contains model_state_dict, optimizer_state_dict

5. **Integration tests**:
   - test_train_gnn_verifier(): Test convenience function end-to-end
   - test_trained_model_accuracy(): After training, accuracy > random (>50%)
   - test_training_history_structure(): Verify history dict has expected keys

6. **Edge cases**:
   - test_small_dataset(): Handle dataset with <batch_size samples
   - test_no_validation(): Training without validation dataset
   - test_single_sample(): Handle batch_size=1

Use small datasets (50-100 samples) and few epochs (2-5) for fast test execution.
Follow existing test patterns from test_gat_verifier.py.
  </action>
  <verify>
pytest tests/test_gnn/test_trainer.py -v --tb=short
  </verify>
  <done>
All training tests pass. GNNTrainer correctly trains GATVerifier, tracks metrics, handles validation, and supports checkpointing.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from fyp.gnn import GNNTrainer, train_gnn_verifier"` succeeds
2. `pytest tests/test_gnn/test_trainer.py -v` all pass
3. Training loop produces decreasing loss
4. Model weights change after training (not frozen)
5. Checkpoints can be saved and loaded
6. Training history contains expected metrics
</verification>

<success_criteria>
- GNNTrainer class implements complete training loop
- BCELoss used for binary node classification
- Adam optimizer with configurable learning rate
- Checkpoint save/load working
- train_gnn_verifier() convenience function works end-to-end
- All tests pass (target: 18+ tests)
</success_criteria>

<output>
After completion, create `.planning/phases/01-gnn-verifier-foundation/01-05-SUMMARY.md`
</output>
