---
phase: 01-gnn-verifier-foundation
plan: 06
type: execute
wave: 3
depends_on: ["01-04", "01-05"]
files_modified:
  - scripts/train_gnn_verifier.py
  - scripts/evaluate_gnn_verifier.py
  - data/derived/models/gnn/.gitkeep
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Evaluation script measures accuracy on held-out test set"
    - "Model achieves >85% accuracy on synthetic anomalies"
    - "Trained model weights are saved and loadable"
    - "Evaluation reports precision, recall, F1 metrics"
  artifacts:
    - path: "scripts/train_gnn_verifier.py"
      provides: "CLI script to train and save GATVerifier"
      min_lines: 80
    - path: "scripts/evaluate_gnn_verifier.py"
      provides: "CLI script to evaluate trained model"
      min_lines: 60
    - path: "data/derived/models/gnn/gnn_verifier_v1.pth"
      provides: "Trained model weights achieving >85% accuracy"
  key_links:
    - from: "scripts/train_gnn_verifier.py"
      to: "src/fyp/gnn/trainer.py"
      via: "import + training execution"
      pattern: "from fyp.gnn import GNNTrainer"
    - from: "scripts/evaluate_gnn_verifier.py"
      to: "src/fyp/gnn/gat_verifier.py"
      via: "import + model loading"
      pattern: "from fyp.gnn import GATVerifier"
---

<objective>
Create training and evaluation scripts that produce trained model weights achieving >85% accuracy on held-out synthetic anomalies.

Purpose: Close the final gaps for Phase 1 Success Criterion #3:
- "Evaluation script that measures accuracy on held-out test set"
- "Trained model weights achieving >85% accuracy"
- "Training metrics logging and checkpointing"

Output: CLI scripts for training and evaluation, plus trained model checkpoint achieving the >85% accuracy target.
</objective>

<execution_context>
@/Users/vatsalmehta/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vatsalmehta/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-gnn-verifier-foundation/01-02-SUMMARY.md
@src/fyp/gnn/gat_verifier.py
@src/fyp/gnn/trainer.py
@src/fyp/gnn/synthetic_dataset.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create training script with CLI</name>
  <files>scripts/train_gnn_verifier.py</files>
  <action>
Create a CLI script to train the GATVerifier model:

```python
#!/usr/bin/env python
"""Train GATVerifier on synthetic anomaly data.

Usage:
    python scripts/train_gnn_verifier.py --epochs 100 --output data/derived/models/gnn/gnn_verifier_v1.pth
"""
import argparse
import logging
import json
from pathlib import Path

from fyp.gnn import GATVerifier, GNNTrainer, SyntheticAnomalyDataset

def main():
    parser = argparse.ArgumentParser(description="Train GATVerifier model")
    parser.add_argument("--epochs", type=int, default=100, help="Training epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--hidden", type=int, default=64, help="Hidden channels")
    parser.add_argument("--train-samples", type=int, default=2000, help="Training samples")
    parser.add_argument("--val-samples", type=int, default=400, help="Validation samples")
    parser.add_argument("--num-nodes", type=int, default=44, help="Nodes per graph")
    parser.add_argument("--temporal-features", type=int, default=5, help="Temporal feature dim")
    parser.add_argument("--output", type=str, default="data/derived/models/gnn/gnn_verifier_v1.pth")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    # Create output directory
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Create datasets
    logger.info(f"Creating synthetic datasets: {args.train_samples} train, {args.val_samples} val")
    train_dataset = SyntheticAnomalyDataset(
        num_samples=args.train_samples,
        num_nodes=args.num_nodes,
        temporal_features=args.temporal_features,
        seed=args.seed,
    )
    val_dataset = SyntheticAnomalyDataset(
        num_samples=args.val_samples,
        num_nodes=args.num_nodes,
        temporal_features=args.temporal_features,
        seed=args.seed + 1000,  # Different seed for validation
    )

    # Create model
    model = GATVerifier(
        temporal_features=args.temporal_features,
        hidden_channels=args.hidden,
    )
    logger.info(f"Created GATVerifier: {model}")

    # Create trainer and train
    trainer = GNNTrainer(
        model=model,
        learning_rate=args.lr,
        checkpoint_dir=str(output_path.parent),
    )

    logger.info(f"Starting training for {args.epochs} epochs...")
    history = trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        log_every=10,
    )

    # Save final model
    trainer.save_checkpoint(str(output_path), args.epochs, history)
    logger.info(f"Saved trained model to {output_path}")

    # Save training history
    history_path = output_path.with_suffix('.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    logger.info(f"Saved training history to {history_path}")

    # Report final metrics
    if 'val_accuracy' in history:
        final_acc = history['val_accuracy'][-1] if history['val_accuracy'] else 0
        best_acc = max(history['val_accuracy']) if history['val_accuracy'] else 0
        logger.info(f"Final validation accuracy: {final_acc:.2%}")
        logger.info(f"Best validation accuracy: {best_acc:.2%}")

if __name__ == "__main__":
    main()
```

Key features:
- CLI arguments for all hyperparameters
- Logging throughout training
- Saves model checkpoint and training history JSON
- Reports final accuracy metrics
- Uses reproducible random seeds
  </action>
  <verify>
python scripts/train_gnn_verifier.py --epochs 5 --train-samples 100 --val-samples 20 --output /tmp/test_model.pth && echo "Training script works"
  </verify>
  <done>
Training script runs end-to-end, saves model checkpoint and training history JSON, reports accuracy metrics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create evaluation script with comprehensive metrics</name>
  <files>scripts/evaluate_gnn_verifier.py</files>
  <action>
Create a CLI script to evaluate trained GATVerifier on held-out test data:

```python
#!/usr/bin/env python
"""Evaluate trained GATVerifier on held-out test set.

Usage:
    python scripts/evaluate_gnn_verifier.py --model data/derived/models/gnn/gnn_verifier_v1.pth --test-samples 500
"""
import argparse
import logging
import json
from pathlib import Path

import torch
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from torch_geometric.loader import DataLoader

from fyp.gnn import GATVerifier, SyntheticAnomalyDataset

def evaluate_model(model, test_loader, threshold=0.5):
    """Evaluate model on test data and compute metrics."""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in test_loader:
            scores = model(batch.x, batch.edge_index, batch.node_type)
            preds = (scores.squeeze() > threshold).int()
            all_preds.extend(preds.cpu().numpy().tolist())
            all_labels.extend(batch.y.cpu().numpy().tolist())

    # Compute metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm.tolist(),
        'num_samples': len(all_labels),
        'num_positive': sum(all_labels),
        'num_negative': len(all_labels) - sum(all_labels),
    }

def main():
    parser = argparse.ArgumentParser(description="Evaluate GATVerifier model")
    parser.add_argument("--model", type=str, required=True, help="Path to trained model checkpoint")
    parser.add_argument("--test-samples", type=int, default=500, help="Number of test samples")
    parser.add_argument("--num-nodes", type=int, default=44, help="Nodes per graph")
    parser.add_argument("--temporal-features", type=int, default=5, help="Temporal feature dim")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size")
    parser.add_argument("--threshold", type=float, default=0.5, help="Classification threshold")
    parser.add_argument("--seed", type=int, default=9999, help="Random seed for test set")
    parser.add_argument("--output", type=str, help="Output JSON path for metrics")
    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    # Load model
    logger.info(f"Loading model from {args.model}")
    checkpoint = torch.load(args.model, map_location='cpu')

    # Infer model config from checkpoint or use defaults
    model = GATVerifier(
        temporal_features=args.temporal_features,
        hidden_channels=64,  # Default from training
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    logger.info("Model loaded successfully")

    # Create held-out test dataset (different seed from train/val)
    logger.info(f"Creating test dataset with {args.test_samples} samples")
    test_dataset = SyntheticAnomalyDataset(
        num_samples=args.test_samples,
        num_nodes=args.num_nodes,
        temporal_features=args.temporal_features,
        seed=args.seed,
    )
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

    # Evaluate
    logger.info("Evaluating model...")
    metrics = evaluate_model(model, test_loader, threshold=args.threshold)

    # Report results
    logger.info("=" * 50)
    logger.info("EVALUATION RESULTS")
    logger.info("=" * 50)
    logger.info(f"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    logger.info(f"Precision: {metrics['precision']:.4f}")
    logger.info(f"Recall:    {metrics['recall']:.4f}")
    logger.info(f"F1 Score:  {metrics['f1']:.4f}")
    logger.info(f"")
    logger.info(f"Test samples: {metrics['num_samples']}")
    logger.info(f"Positive (anomaly): {metrics['num_positive']}")
    logger.info(f"Negative (normal): {metrics['num_negative']}")
    logger.info("=" * 50)

    # Check against target
    target_accuracy = 0.85
    if metrics['accuracy'] >= target_accuracy:
        logger.info(f"SUCCESS: Accuracy {metrics['accuracy']:.2%} >= {target_accuracy:.0%} target")
    else:
        logger.warning(f"BELOW TARGET: Accuracy {metrics['accuracy']:.2%} < {target_accuracy:.0%} target")

    # Save metrics if output path provided
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        logger.info(f"Saved metrics to {output_path}")

    return metrics['accuracy'] >= target_accuracy

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
```

Key features:
- Loads trained model from checkpoint
- Creates held-out test set with different seed
- Computes accuracy, precision, recall, F1, confusion matrix
- Checks against 85% accuracy target
- Returns exit code 0 if target met, 1 if not
- Optionally saves metrics to JSON
  </action>
  <verify>
python scripts/evaluate_gnn_verifier.py --model /tmp/test_model.pth --test-samples 50 && echo "Evaluation script works"
  </verify>
  <done>
Evaluation script loads model, runs inference on held-out test set, reports comprehensive metrics, checks against 85% target.
  </done>
</task>

<task type="auto">
  <name>Task 3: Train and evaluate model to achieve >85% accuracy</name>
  <files>data/derived/models/gnn/.gitkeep</files>
  <action>
1. Create the model output directory:
```bash
mkdir -p data/derived/models/gnn
touch data/derived/models/gnn/.gitkeep
```

2. Train the GATVerifier model with sufficient data and epochs:
```bash
python scripts/train_gnn_verifier.py \
    --epochs 100 \
    --train-samples 2000 \
    --val-samples 400 \
    --batch-size 32 \
    --lr 0.001 \
    --hidden 64 \
    --num-nodes 44 \
    --seed 42 \
    --output data/derived/models/gnn/gnn_verifier_v1.pth
```

3. Evaluate on held-out test set (different seed):
```bash
python scripts/evaluate_gnn_verifier.py \
    --model data/derived/models/gnn/gnn_verifier_v1.pth \
    --test-samples 500 \
    --seed 9999 \
    --output data/derived/models/gnn/evaluation_results.json
```

4. Verify >85% accuracy achieved. If not:
   - Increase training samples to 5000
   - Increase epochs to 200
   - Try learning rate 0.0005
   - Retrain and re-evaluate

5. Once >85% accuracy confirmed, the model checkpoint is production-ready.

Note: The synthetic dataset is designed to produce learnable patterns. With proper training hyperparameters, >85% accuracy should be achievable. The GATVerifier architecture (GATv2Conv with oversmoothing prevention) is well-suited for this task.
  </action>
  <verify>
python scripts/evaluate_gnn_verifier.py --model data/derived/models/gnn/gnn_verifier_v1.pth --test-samples 500 --seed 9999 && echo "Target accuracy achieved!"
  </verify>
  <done>
Trained model checkpoint exists at data/derived/models/gnn/gnn_verifier_v1.pth with >85% accuracy on held-out test set. Phase 1 Success Criterion #3 is now satisfied.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/train_gnn_verifier.py --help` shows CLI options
2. `python scripts/evaluate_gnn_verifier.py --help` shows CLI options
3. Training completes and saves checkpoint
4. Evaluation reports accuracy >= 85%
5. `data/derived/models/gnn/gnn_verifier_v1.pth` exists
6. `data/derived/models/gnn/evaluation_results.json` shows metrics
</verification>

<success_criteria>
- Training script trains model end-to-end
- Evaluation script measures accuracy on held-out test set
- Trained model achieves >85% accuracy on synthetic anomalies
- Model checkpoint saved to data/derived/models/gnn/gnn_verifier_v1.pth
- Evaluation metrics saved to JSON
- Phase 1 Success Criterion #3 is fully satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/01-gnn-verifier-foundation/01-06-SUMMARY.md`
</output>
