---
phase: 01-gnn-verifier-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/fyp/gnn/temporal_encoder.py
  - src/fyp/gnn/gat_verifier.py
  - src/fyp/gnn/__init__.py
autonomous: true

must_haves:
  truths:
    - "Temporal features can be encoded into fixed-dimension embeddings"
    - "GAT model processes graph-structured input and outputs per-node anomaly scores"
    - "Model prevents oversmoothing (embeddings remain distinguishable across layers)"
    - "Inference latency is under 30ms for batch_size=32"
  artifacts:
    - path: "src/fyp/gnn/temporal_encoder.py"
      provides: "1D-Conv time series encoder"
      min_lines: 50
      exports: ["TemporalEncoder"]
    - path: "src/fyp/gnn/gat_verifier.py"
      provides: "GAT-based anomaly verifier model"
      min_lines: 100
      exports: ["GATVerifier"]
      contains: "GATv2Conv"
  key_links:
    - from: "src/fyp/gnn/gat_verifier.py"
      to: "torch_geometric.nn.GATv2Conv"
      via: "import and layer instantiation"
      pattern: "from torch_geometric.nn import GATv2Conv"
    - from: "src/fyp/gnn/gat_verifier.py"
      to: "src/fyp/gnn/temporal_encoder.py"
      via: "import and composition"
      pattern: "from.*temporal_encoder import TemporalEncoder"
---

<objective>
Implement the temporal encoder and GAT-based verifier model that processes graph-structured grid data and outputs per-node anomaly scores.

Purpose: Creates the core GNN architecture that understands grid topology and can score anomalies based on spatial relationships between nodes.

Output: Working GATVerifier model with temporal encoding, skip connections for oversmoothing prevention, and anomaly score output.
</objective>

<execution_context>
@/Users/vatsalmehta/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vatsalmehta/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-gnn-verifier-foundation/01-CONTEXT.md
@.planning/phases/01-gnn-verifier-foundation/01-RESEARCH.md

# Prior plan output
@.planning/phases/01-gnn-verifier-foundation/01-01-SUMMARY.md

# Reference for existing model patterns
@src/fyp/models/autoencoder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement TemporalEncoder</name>
  <files>src/fyp/gnn/temporal_encoder.py</files>
  <action>
Implement 1D-Conv temporal encoder that processes time-window features per node.

**Class structure:**
```python
class TemporalEncoder(nn.Module):
    """1D-Conv encoder for fixed time-window features.

    Processes temporal features like [current_load, avg_24h, peak_7d]
    into a dense embedding per node. Uses 1D convolution for efficient
    local pattern extraction (faster than LSTM, per research).
    """

    def __init__(
        self,
        input_features: int,
        embed_dim: int = 64,
        kernel_size: int = 3,
        dropout: float = 0.1,
    ):
        """Initialize encoder.

        Args:
            input_features: Number of temporal features (e.g., 3 for current/avg/peak)
            embed_dim: Output embedding dimension (default 64, matches GNN hidden)
            kernel_size: Conv kernel size (default 3)
            dropout: Dropout rate (default 0.1)
        """

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Encode temporal features.

        Args:
            x: [num_nodes, num_temporal_features]

        Returns:
            [num_nodes, embed_dim]
        """
```

**Implementation details:**

1. **Architecture:**
   - If input_features >= 3: Use 1D convolution pipeline
   - If input_features < 3: Use simple linear fallback (conv needs minimum size)
   - Conv pipeline: Conv1d(1, 32) -> ReLU -> Conv1d(32, embed_dim) -> ReLU -> AdaptiveAvgPool1d(1)

2. **Key patterns:**
   - Input shape: [N, F] -> reshape to [N, 1, F] for Conv1d
   - Output shape: [N, embed_dim, 1] -> squeeze to [N, embed_dim]
   - Apply LayerNorm at output for stable gradient flow

3. **Handle edge cases:**
   - Very small feature sets use linear projection
   - Add dropout after convolutions for regularization

**Reference from research:**
```python
# 1D convolution treats features as sequence
self.conv1 = nn.Conv1d(1, 32, kernel_size=min(kernel_size, input_features), padding='same')
self.conv2 = nn.Conv1d(32, embed_dim, kernel_size=min(kernel_size, input_features), padding='same')
self.pool = nn.AdaptiveAvgPool1d(1)
self.norm = nn.LayerNorm(embed_dim)
```
  </action>
  <verify>
```bash
poetry run python -c "
import torch
from fyp.gnn.temporal_encoder import TemporalEncoder

# Test with typical temporal features
encoder = TemporalEncoder(input_features=5, embed_dim=64)
x = torch.randn(100, 5)  # 100 nodes, 5 temporal features
out = encoder(x)
print(f'Input: {x.shape}, Output: {out.shape}')
assert out.shape == (100, 64), f'Expected (100, 64), got {out.shape}'

# Test with small feature set (linear fallback)
encoder_small = TemporalEncoder(input_features=2, embed_dim=64)
x_small = torch.randn(50, 2)
out_small = encoder_small(x_small)
assert out_small.shape == (50, 64)

print('SUCCESS')
"
  </verify>
  <done>TemporalEncoder produces [num_nodes, embed_dim] output from temporal features</done>
</task>

<task type="auto">
  <name>Task 2: Implement GATVerifier</name>
  <files>src/fyp/gnn/gat_verifier.py</files>
  <action>
Implement GAT-based anomaly verifier with oversmoothing prevention.

**Class structure:**
```python
class GATVerifier(nn.Module):
    """GAT-based anomaly verifier with oversmoothing prevention.

    Uses GATv2Conv (not GATConv) for dynamic attention, following
    research recommendations. Implements GCNII-style initial residual
    connections to prevent oversmoothing in 3-layer architecture.

    Architecture:
    1. Temporal encoding (1D-Conv) per node
    2. Optional node type embedding
    3. 3x GATv2Conv layers with residuals + LayerNorm
    4. Output head -> per-node anomaly score in [0, 1]
    """

    def __init__(
        self,
        temporal_features: int,
        hidden_channels: int = 64,
        num_layers: int = 3,
        heads: int = 4,
        dropout: float = 0.1,
        num_node_types: int = 3,
    ):
        """Initialize verifier.

        Args:
            temporal_features: Number of input temporal features
            hidden_channels: Hidden dimension (default 64 per CONTEXT.md)
            num_layers: Number of GAT layers (default 3 per CONTEXT.md)
            heads: Attention heads per layer (default 4 per research)
            dropout: Dropout rate
            num_node_types: Number of node types for embedding (default 3: substation/feeder/household)
        """

    def forward(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        node_type: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """Forward pass.

        Args:
            x: Node temporal features [num_nodes, temporal_features]
            edge_index: Graph connectivity [2, num_edges]
            node_type: Node type indices [num_nodes] (optional)

        Returns:
            Anomaly scores [num_nodes, 1] in range [0, 1]
        """
```

**Implementation details:**

1. **Input processing:**
   - Pass temporal features through TemporalEncoder
   - If node_type provided, add learnable type embedding (16-dim)
   - Concatenate: [temporal_embed, type_embed] -> project to hidden_channels

2. **GAT layers (from research):**
   ```python
   # Each layer
   conv = GATv2Conv(
       in_channels=hidden_channels,
       out_channels=hidden_channels // heads,  # Concatenated = hidden_channels
       heads=heads,
       concat=True,
       dropout=dropout,
       add_self_loops=True,
       residual=True,  # Built-in residual
   )
   ```

3. **Oversmoothing prevention (GCNII-style):**
   ```python
   # Learnable initial residual weight
   self.alpha = nn.Parameter(torch.tensor(0.5))

   # In forward:
   h0 = initial_projection(x)  # Store initial
   for conv, norm in zip(self.convs, self.norms):
       h_new = conv(h, edge_index)
       h_new = self.alpha * h_new + (1 - self.alpha) * h0  # Initial residual
       h = norm(h_new)
   ```

4. **Output head:**
   ```python
   self.output_head = nn.Sequential(
       nn.Linear(hidden_channels, hidden_channels // 2),
       nn.ReLU(),
       nn.Dropout(dropout),
       nn.Linear(hidden_channels // 2, 1),
       nn.Sigmoid(),  # Anomaly score in [0, 1]
   )
   ```

5. **Important anti-patterns to avoid:**
   - Do NOT use GATConv (use GATv2Conv for dynamic attention)
   - Do NOT skip LayerNorm after GAT layers
   - Do NOT forget initial residual connections
   - Do NOT average attention heads in middle layers (use concat=True)

**Inference optimization:**
- Model should work with torch.inference_mode()
- Design for batch processing (multiple graphs via PyG DataLoader)
  </action>
  <verify>
```bash
poetry run python -c "
import torch
from fyp.gnn.gat_verifier import GATVerifier

# Create model
model = GATVerifier(
    temporal_features=5,
    hidden_channels=64,
    num_layers=3,
    heads=4,
)
model.eval()

# Test forward pass
num_nodes = 100
x = torch.randn(num_nodes, 5)  # Temporal features
edge_index = torch.randint(0, num_nodes, (2, 300))  # Random edges
node_type = torch.randint(0, 3, (num_nodes,))  # Node types

with torch.inference_mode():
    scores = model(x, edge_index, node_type)

print(f'Input: {x.shape}')
print(f'Output: {scores.shape}')
print(f'Score range: [{scores.min():.3f}, {scores.max():.3f}]')

assert scores.shape == (num_nodes, 1), f'Expected ({num_nodes}, 1), got {scores.shape}'
assert scores.min() >= 0 and scores.max() <= 1, 'Scores should be in [0, 1]'

print('SUCCESS')
"
  </verify>
  <done>GATVerifier produces per-node anomaly scores in [0,1] with proper architecture</done>
</task>

<task type="auto">
  <name>Task 3: Update module exports and verify latency</name>
  <files>src/fyp/gnn/__init__.py</files>
  <action>
Update the gnn module __init__.py to export new components and verify inference latency target.

1. **Update exports:**
   ```python
   from .graph_builder import GridGraphBuilder
   from .temporal_encoder import TemporalEncoder
   from .gat_verifier import GATVerifier

   __all__ = ["GridGraphBuilder", "TemporalEncoder", "GATVerifier"]
   ```

2. **Run latency benchmark** to verify <30ms target for batch_size=32:
   ```python
   # In a separate test script or inline verification
   import time
   import torch

   model = GATVerifier(temporal_features=5)
   model.eval()

   # Simulate batch of 32 graphs with ~100 nodes each
   x = torch.randn(3200, 5)  # 32 * 100 nodes
   edge_index = torch.randint(0, 3200, (2, 10000))

   # Warmup
   with torch.inference_mode():
       _ = model(x, edge_index)

   # Benchmark
   times = []
   for _ in range(100):
       start = time.perf_counter()
       with torch.inference_mode():
           _ = model(x, edge_index)
       times.append((time.perf_counter() - start) * 1000)

   avg_ms = sum(times) / len(times)
   print(f'Average latency: {avg_ms:.2f}ms')
   assert avg_ms < 30, f'Latency {avg_ms:.2f}ms exceeds 30ms target'
   ```

If latency exceeds target, consider:
- Reducing heads from 4 to 2
- Reducing hidden_channels from 64 to 32
- But first try without changes - 64-dim 3-layer GAT should be fast enough
  </action>
  <verify>
```bash
poetry run python -c "
import time
import torch
from fyp.gnn import GridGraphBuilder, TemporalEncoder, GATVerifier

# Verify all exports work
print('Imports OK')

# Latency test
model = GATVerifier(temporal_features=5, hidden_channels=64, num_layers=3, heads=4)
model.eval()

# Simulate batch_size=32 with ~100 nodes per graph
x = torch.randn(3200, 5)
edge_index = torch.randint(0, 3200, (2, 10000))
node_type = torch.randint(0, 3, (3200,))

# Warmup
with torch.inference_mode():
    _ = model(x, edge_index, node_type)

# Benchmark
times = []
for _ in range(50):
    start = time.perf_counter()
    with torch.inference_mode():
        _ = model(x, edge_index, node_type)
    times.append((time.perf_counter() - start) * 1000)

avg_ms = sum(times) / len(times)
print(f'Latency (batch=32, ~100 nodes/graph): {avg_ms:.2f}ms')

if avg_ms < 30:
    print('SUCCESS: Under 30ms target')
else:
    print(f'WARNING: {avg_ms:.2f}ms exceeds 30ms target')
"
  </verify>
  <done>All GNN components exported and inference latency verified</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Full module verification:**
```bash
poetry run python -c "
from fyp.gnn import GridGraphBuilder, TemporalEncoder, GATVerifier
import torch

# Build a graph
import pandas as pd
df = pd.DataFrame({
    'primary_substation_id': ['PS1'] * 5,
    'secondary_substation_id': ['SS1', 'SS1', 'SS2', 'SS2', 'SS2'],
    'lv_feeder_id': ['LV1', 'LV2', 'LV3', 'LV4', 'LV5'],
    'total_mpan_count': [50, 30, 40, 20, 60],
})

builder = GridGraphBuilder()
data = builder.build_from_metadata(df)
print(f'Graph: {data.num_nodes} nodes, {data.edge_index.size(1)} edges')

# Create model and run inference
model = GATVerifier(temporal_features=data.x.size(1))
model.eval()

with torch.inference_mode():
    scores = model(data.x, data.edge_index, data.node_type)

print(f'Anomaly scores: {scores.squeeze().tolist()}')
print('Full pipeline SUCCESS')
"
```

2. **Check oversmoothing prevention:**
```bash
poetry run python -c "
import torch
from fyp.gnn import GATVerifier

model = GATVerifier(temporal_features=5)
model.eval()

# Create graph with distinct node types
x = torch.randn(30, 5)
# Linear topology: 0-1-2-...-29
edge_index = torch.stack([
    torch.arange(29),
    torch.arange(1, 30)
])
edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # Bidirectional

node_type = torch.tensor([0]*10 + [1]*10 + [2]*10)

with torch.inference_mode():
    scores = model(x, edge_index, node_type)

# Check that different node types have different score distributions
type0_scores = scores[node_type == 0].mean()
type1_scores = scores[node_type == 1].mean()
type2_scores = scores[node_type == 2].mean()

print(f'Type 0 (substation) mean score: {type0_scores:.4f}')
print(f'Type 1 (feeder) mean score: {type1_scores:.4f}')
print(f'Type 2 (household) mean score: {type2_scores:.4f}')

# Scores should not be identical (oversmoothing would make them converge)
score_std = scores.std()
print(f'Score std: {score_std:.4f}')
assert score_std > 0.01, 'Scores too uniform - possible oversmoothing'
print('Oversmoothing check PASSED')
"
```
</verification>

<success_criteria>
- [ ] TemporalEncoder produces correct output shape [num_nodes, embed_dim]
- [ ] GATVerifier uses GATv2Conv (not GATConv)
- [ ] GATVerifier has 3 layers with 4 heads each
- [ ] Initial residual connections implemented for oversmoothing prevention
- [ ] LayerNorm after each GAT layer
- [ ] Output is per-node anomaly score in [0, 1]
- [ ] Inference latency < 30ms for batch_size=32
- [ ] All components exported from fyp.gnn module
</success_criteria>

<output>
After completion, create `.planning/phases/01-gnn-verifier-foundation/01-02-SUMMARY.md`
</output>
